# Deep Reinforcement Learning (Deep RL) 
- combines Reinforcement Learning (RL) and Deep Learning, enabling agents to learn complex decision-making strategies from raw, unstructured data, such as images, without manual feature engineering. 

## Overview:
•	Reinforcement Learning (RL): RL is a machine learning approach where an agent learns to make decisions by interacting with an environment, receiving rewards or penalties for its actions, and iteratively optimizing its actions to maximize cumulative reward.      
•	Deep Learning: Deep learning uses artificial neural networks with multiple layers to learn complex patterns from data.      
•	Deep Reinforcement Learning (Deep RL): Deep RL leverages deep learning to represent and learn the value function or policy of an RL agent, allowing it to handle high-dimensional and complex environments.  
    
## Key Concepts in Deep RL:
◦	Agent: The entity that interacts with the environment and makes decisions.  
◦	Environment: The context in which the agent operates.  
◦	State: The current situation or information the agent has about the environment.  
◦	Action: The decision or step the agent takes.  
◦	Reward: A signal indicating the desirability of a particular action or state.  
◦	Policy: The strategy the agent uses to select actions.  
◦	Value Function: A function that estimates the expected cumulative reward for a given state or state-action pair.  

## Deep RL Algorithms:
◦	Deep Q-Network (DQN): Uses a deep neural network to approximate the Q-function, which estimates the value of taking a specific action in a given state.  
◦	Policy Gradient Methods: Directly learn the policy by optimizing the probability of taking certain actions.  
◦	Actor-Critic Methods: Combine policy gradient and value function methods to improve learning efficiency. 


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++


# RLAIF vs RLHF

Replace Human Feedback with AI Feedback? 

Google DeepMind released a paper called “RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback” to compare RLHF to RLAIF on text summarization to evaluate if AI-generated feedback (RLAIF) can replace Human Feedback (RLHF) for LLM alignment. 

🤖 RLAIF Implementations
1️⃣ Started from the TL;DR summarization dataset based on Reddit posts.
2️⃣ Used PaLM2 (already RLHFed) to assign a preference label for a text and two candidate summaries. Used the log probabilities for the candidates as scores. To mitigate positional bias, the made two generations with reversed order.
3️⃣ Trained base SFT Model on a smaller dataset
4️⃣ Trained a Reward Model (RM) initialized from the SFT on AI preferences with a contrastive loss for 3 epochs
5️⃣ Fine-tuned a policy with reinforcement learning initialized from the SFT for 1 million episodes or 8 epochs

✨ 𝗜𝗻𝘀𝗶𝗴𝗵𝘁𝘀:
⚖️ Humans equally prefer RLAIF and RLHF summaries over SFT ~70% of the time.
🥊 High-quality human/ai labels are key for scaling up RLHF
📈 Prompting techniques such as CoT and larger LLM improve AI feedback.
📍 Found evidence for positional bias, tried to mitigate by switching the order
🤔 Used PaLM 2 (already RLHFed) for labeling preferences
💬 RLAIF and RLHF policies tend to generate longer summaries than the SFT policy
🥿 AI RM quickly plateaus after a few thousand examples, and HF RM continues to improve with more data
🧐 Comparison SFT model was trained only for a single epoch
🔍 Observed is that RLAIF appears less likely to hallucinate than RLHF

Full paper: https://arxiv.org/abs/2309.00267



++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
